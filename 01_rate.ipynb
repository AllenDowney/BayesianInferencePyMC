{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Estimating Rates\n",
    "\n",
    "[Bayesian Inference with PyMC](https://allendowney.github.io/BayesianInferencePyMC)\n",
    "\n",
    "Copyright 2021 Allen B. Downey\n",
    "\n",
    "License: [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we're running on Colab, install PyMC and ArviZ\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install pymc3\n",
    "    !pip install arviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyMC generates a FutureWarning we don't need to deal with yet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll use the result of a hockey game to estimate the rate at which each team would score against the other, in the long run.\n",
    "In the next notebook, we'll use these estimates to make predictions.\n",
    "\n",
    "This example is meant to be fun, and to take advantage of our intuition for things like games, but the methods we'll see are useful for any application where we are interested in events that occur over time, like customers arriving at a store, or patients being diagnosed with a disease.\n",
    "\n",
    "Many systems like this can be well modeled by a Poisson process, which is based on the assumption that events are random and equally likely to occur at any time.\n",
    "Of course, this assumption is not always valid, but it is often good enough for modeling.\n",
    "\n",
    "In the example of scoring in games like hockey and soccer (football), it turns out to be a pretty good model, [according to this paper](https://arxiv.org/abs/1002.0797)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hockey\n",
    "\n",
    "In a hockey game, let's assume that each team has some long-term average scoring rate, $\\mu$, in goals per game.\n",
    "\n",
    "To get started, suppose that, for Team A versus Team B, $\\mu$ is somehow known to be 2.4 goals per game.\n",
    "If these two teams played 100 games, we would expect Team A to score 2.4 goals per game, on average.\n",
    "\n",
    "But in an particular game, they might score 2 or 3 goals, or more, or less.\n",
    "Based on the assumptions of a Poisson process, it turns out, the distribution of goals scored is Poisson with parameter `mu=2.4`.\n",
    "\n",
    "To see what that distribution looks like, I'll generate a random sample and plot a histogram.\n",
    "Normally I would use NumPy or SciPy to generate random values from a particular distribution, but since we'll be using PyMC later, we'll start using it now.\n",
    "Here's how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what the histogram looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_hist(sample, **options):\n",
    "    \"\"\"Plot a histogram of goals.\n",
    "    \n",
    "    sample: sequence of values\n",
    "    \"\"\"\n",
    "    sns.histplot(sample, stat='probability', discrete=True,\n",
    "                 alpha=0.5, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def legend(**options):\n",
    "    \"\"\"Make a legend only if there are labels.\"\"\"\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    if len(labels):\n",
    "        plt.legend(**options)\n",
    "        \n",
    "def decorate_goals(ylabel='Probability'):\n",
    "    \"\"\"Decorate the axes.\"\"\"\n",
    "    plt.xlabel('Number of goals')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title('Distribution of goals scored')\n",
    "    legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(sample_poisson)\n",
    "decorate_goals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the average is 2.4, it is most common to score 1 or 2 goals, less common to score 3 or 4, less common to score 0, and rare to score more than 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gamma distribution\n",
    "\n",
    "In the previous section we assume that the goal scoring rate is known, but usually it's something we have to estimate.\n",
    "If we are not certain what the value of `mu` is, we can use a distribution to represent possible values and their relative likelihoods.\n",
    "\n",
    "To represent this distribution, I will use a gamma distribution.\n",
    "That's not the only option, but the gamma distribution is a reasonable choice because:\n",
    "\n",
    "* The values in a gamma distribution are continuous and non-negative, so that's appropriate for representing a goal-scoring rate.\n",
    "\n",
    "* As we'll see, the shape of the gamma distribution is consistent with what we know about goal scoring in games like hockey.\n",
    "\n",
    "The shape of the gamma distribution is controlled by two parameters, `alpha` and `beta`.\n",
    "I'll assign them values now, and at the end of the fourth notebook, we'll see where they came from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 4.6\n",
    "beta = 1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use PyMC again to draw a sample from this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_kde(sample, **options):\n",
    "    \"\"\"Plot a distribution using KDE.\n",
    "    \n",
    "    sample: sequence of values\n",
    "    \"\"\"\n",
    "    sns.kdeplot(sample, cut=0, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_rate(ylabel='Likelihood'):\n",
    "    \"\"\"Decorate the axes.\"\"\"\n",
    "    plt.xlabel('Goals per game (mu)')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title('Distribution of goal scoring rate')\n",
    "    legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde(sample_gamma, label='gamma')\n",
    "decorate_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution represents possible goal-scoring rates for one team against another.\n",
    "When the worst team plays the best team, their goal-scoring rate might be close to 0.\n",
    "When the best team plays the worst, their goal-scoring rate might be 8 goals per game, or more.\n",
    "But for most teams, we expect goal-scoring rates near 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a model\n",
    "\n",
    "As we've seen, we can use PyMC to generate random values from a distribution.\n",
    "But it is more common to use PyMC to create a `Model`, which is an object that represents random quantities and the relationships between them.\n",
    "\n",
    "We'll start with a model that contains only one random quantity, and work our way up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line creates an empty model.\n",
    "The `with` statement creates a context so that the quantities we creates inside are added to the model.\n",
    "\n",
    "This model contains a `Gamma` distribution with the name `mu` and the parameters `alpha` and `beta`.\n",
    "\n",
    "Then it uses `sample_prior_predictive` to generate a sample from this distribution.\n",
    "I'll explain the name of this function soon.\n",
    "\n",
    "The result is a \"trace\", which is like a dictionary that maps from names to samples.\n",
    "If we select the name `mu`, we get a sample from the gamma distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the gamma distribution is the prior distribution of `mu`; that is, it represents our knowledge of goal-scoring rates in general, rather than the goal-scoring rate of one team against another.\n",
    "\n",
    "In other words, if we don't know which two teams are playing, this distribution represents what we would believe about `mu`.\n",
    "And here's what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde(sample_prior, color='gray', label='prior')\n",
    "decorate_rate('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than random variation, it looks the same as the distribution we generated in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding to the model\n",
    "\n",
    "Again, a model represents random quantities and the connections between them.\n",
    "So let's add another quantity and a connection.\n",
    "\n",
    "Here's a model that represents the distribution of `mu` *and* the distribution of goals scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous model, `mu` is a goal-scoring rate drawn from a gamma distribution.\n",
    "And now `goals` is the number of goals scored in a particular game, drawn from a Poisson distribution.\n",
    "\n",
    "And, crucially, notice the connection between them: the parameter of the Poisson distribution is the value of `mu`drawn from the gamma distribution.\n",
    "\n",
    "PyMC provides a function that represents a model as a graph that shows the connections between quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `mu` is a rate in goals per game, and `goals` is an integer number of goals.\n",
    "\n",
    "The distribution of `mu` from this model is the **prior distribution**, which we saw in the previous section.\n",
    "\n",
    "The distribution of `goals` is the **prior predictive distribution**, because it represents the number of goals we would predict, based on the prior distribution of `mu`.\n",
    "\n",
    "Let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prior_pred = trace2['goals']\n",
    "sample_prior_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(sample_prior_pred, label='prior pred')\n",
    "decorate_goals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior predictive distribution represents two sources of uncertainty:\n",
    "\n",
    "* We are not sure what the goal-scoring rate, `mu`, is.\n",
    "\n",
    "* Even if we knew `mu`, we would not be sure how many goals would be scored in a particular game.\n",
    "\n",
    "Compared to the Poisson distribution we started with, with known `mu`, the mean of the prior predictive sample is about the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_poisson.mean(), sample_prior_pred.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the standard deviation is bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_poisson.std(), sample_prior_pred.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior predictive distribution shows the distribution of goals we should expect in a game, based on the prior distribution only, not the data.\n",
    "\n",
    "It is useful for model-checking, but it is also a step along the way to inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When do we get to inference?\n",
    "\n",
    "So far, we have been modeling the system *forward*; that is, given the distributions and their parameters, we've been generating possible outcomes.\n",
    "\n",
    "Now we'll start running the model *backward*; given an actual outcome, we will estimate what the parameters might have been.\n",
    "This process is called **inference**.\n",
    "\n",
    "And here's where we discover the power of PyMC. \n",
    "Since we have already specified the model, we can use it for inference with just two changes:\n",
    "\n",
    "* Instead of generating `goals`, we'll mark goals as `observed` and provide the observed data.\n",
    "\n",
    "* And instead of calling `sample_prior_predictive`, we'll call `sample`.\n",
    "\n",
    "For data, let's assume that Team A plays Team B and scores 4 goals.\n",
    "Here's how we add it to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output provides information about how the sampling process worked.\n",
    "The result is a `trace` object similar to the one from the last model.\n",
    "The differences are:\n",
    "\n",
    "* Since we provided observed data for `goals`, the trace does not contain a random sample of goals.\n",
    "\n",
    "* The values of `mu` in the trace are a sample from the **posterior distribution**, which represents what we believe about `mu` based on the data.\n",
    "\n",
    "We select the sample from the trace in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trace3['mu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_posterior = trace3['mu']\n",
    "sample_posterior.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the posterior distribution is higher than the mean of the prior.\n",
    "And here's what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde(sample_prior, label='Prior', color='gray')\n",
    "plot_kde(sample_posterior, label='Posterior')\n",
    "decorate_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the prior distribution is about 2.4.\n",
    "The team scored 4 goals, which is more than expected, so the posterior distribution is shifted to the right.\n",
    "\n",
    "The mean of the posterior distribution is about 3, so it falls between the prior expectation and the observed outcome.\n",
    "\n",
    "We can also use ArviZ to visualize the posterior distribution and compute summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 94% HDI is a **credible interval** that covers 94% of the posterior distribution. Based on the outcome of a single game, there is a 94% chance that the actual goal-scoring rate falls in this range (at least under the assumptions of the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** The example we've run so far is based on three parameters: the values of `alpha` and `beta` that determine the prior distribution, and the observed number of goals.\n",
    "\n",
    "Modify any of these variables and run the example again to see what effect they have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two teams\n",
    "\n",
    "We can extend the model to estimate different values of `mu` for two teams.\n",
    "\n",
    "As an example, suppose Team A scores 5 goals and Team B scores 1. The following model estimates their goal scoring rates against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 4.6\n",
    "beta = 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model4:\n",
    "    mu_A = pm.Gamma('mu_A', alpha, beta)\n",
    "    mu_B = pm.Gamma('mu_B', alpha, beta)\n",
    "    goals_A = pm.Poisson('goals_A', mu_A, observed=[5])\n",
    "    goals_B = pm.Poisson('goals_B', mu_B, observed=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the graphical representation of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can run the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model4:\n",
    "    trace4 = pm.sample(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `plot_trace` to review the results and do some visual diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things to check for:\n",
    "\n",
    "* Since each sampling chain is trying to sample from the same distribution, their results should be similar. If there are large deviations between them, that can be a sign of trouble.\n",
    "\n",
    "* The sequence of values should looks like random noise; if it looks like the sampler gets stuck for a while, that's a sign of trouble, too.\n",
    "\n",
    "For this model, I expect the results to look pretty good.\n",
    "\n",
    "Here are the posterior distribitions for `mu_B` and `mu_A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_A = trace4['mu_A']\n",
    "mu_B = trace4['mu_B']\n",
    "mu_B.mean(), mu_A.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde(mu_A, label='mu_A posterior')\n",
    "plot_kde(mu_B, label='mu_B posterior')\n",
    "decorate_rate('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of superiority\n",
    "\n",
    "On the basis of one game, how confident should we be that Team A is the better team?\n",
    "One way to answer this question is to compute the **probability of superiority**, which is the chance that a random value from the Team A posterior distribution exceeds a random value from the Team B posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a 5-1 result, we have some confidence that Team A is better, but it is not certain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Suppose the same teams play again and Team A wins 3-1.\n",
    "Modify the previous example to take into account both games. How much does the second game affect the posterior distributions and the probability of superiority?\n",
    "\n",
    "Hint: The observed data can be a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
